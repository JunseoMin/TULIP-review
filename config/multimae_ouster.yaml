# Input and output tasks
in_domains: depth-intensity
out_domains: depth-intensity

# Architecture
model: pretrain_multimae_base
decoder_dim: 256
#input_size: 128 * 2048
patch_size: 8
alphas: 1.0  # Dirichlet concentration parameter
num_encoded_tokens: 384      # (128/8)*(128/8)*2 = 512 * 0.75 = 384 (mask ratio = 0.25)          # Total would be (2048/16)*(128/16) *2 =128*8*2 = 2048 (*0.25 = 512) #
num_global_tokens: 1
decoder_use_task_queries: True
decoder_depth: 2

# Train
epochs: 1000
opt: adamw
blr: 0.0001 # this is base_lr = 1e-4, lr = base_lr * batch_size / 256
warmup_lr: 0.000001 # 1e-6
min_lr: 0.
warmup_epochs: 1
batch_size: 16
hflip: 0.5
loss_on_unmasked: True
save_ckpt_freq: 100
# fp32_output_adapters: semseg

# Data
data_path: '/cluster/work/riner/users/biyang/dataset/depth_intensity_middle' # Change me

# Misc
find_unused_params: False

# Wandb logging
log_wandb: True # Set to True to log to Weights & Biases
wandb_project: 'MultiMAE'
wandb_entity: biyang # Change if needed
wandb_run_name: multimae_unmasked_loss_1000epochs
output_dir: 'experiment/durlar/MultiMAE/multimae_with_unmaksed_loss_1000epochs' # Change directory if needed
eval: True
